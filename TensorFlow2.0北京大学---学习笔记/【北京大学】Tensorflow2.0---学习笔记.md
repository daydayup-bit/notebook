ã€åŒ—äº¬å¤§å­¦ã€‘Tensorflow2.0---å­¦ä¹ ç¬”è®°

[ã€åŒ—äº¬å¤§å­¦ã€‘Tensorflow2.0]( https://search.bilibili.com/all?keyword=TensorFlow2.0&from_source=webtop_search&spm_id_from=333.851)

# Tensorflowç‰ˆæœ¬é€‰æ‹©åŠå®‰è£…

[TensorFlowçš„ç‰ˆæœ¬é€‰æ‹©å’Œå®‰è£…](https://www.cnblogs.com/suanai/p/14300090.html)

[CUDAä¸æ˜¾å¡é©±åŠ¨](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html)

[TensorFlowä¸CUDAã€cuDNNç‰ˆæœ¬å¯¹åº”å…³ç³»](https://tensorflow.google.cn/install/source_windows?hl=en#gpu)

**å®‰è£…å‰è¦æ³¨æ„çœ‹è‡ªå·±çš„æ˜¾å¡æ”¯æŒcudaã€cudnnå“ªä¸ªç‰ˆæœ¬**ï¼Œè¿™é‡Œæˆ‘çš„æ˜¾å¡æ˜¯3090ï¼Œå®‰è£…TensorFlow2.5ï¼Œcuda11.2ï¼Œcudnn8.1

1.anaconda promptè¾“å…¥`conda create -n TF2.5 python=3.8`æ–°å»ºTF2.5çš„ç¯å¢ƒ

2.`conda activate TF2.5`è¿›å…¥TF2.5ç¯å¢ƒ

3.`conda install cudatoolkit=11.2`å®‰è£…è‹±ä¼Ÿè¾¾SDK11.2

è¿™ä¸€æ­¥å¯èƒ½ä¼šå‡ºç°ä¸‹å›¾çš„æŠ¥é”™

![image-20210913161049452](img/image-20210913161049452.png)

è§£å†³æ–¹æ³•ï¼šè¾“å…¥`conda config --add channels conda-forge`

4.å®‰è£…è‹±ä¼Ÿè¾¾æ·±åº¦å­¦ä¹ è½¯ä»¶åŒ…

`conda install cudnn=8.1`

5.å®‰è£…TensorFlow

`pip install tensorflow==2.5`æŒ‡å®š2.5ç‰ˆæœ¬

6.éªŒè¯æ˜¯å¦å®‰è£…æˆåŠŸ

```python
import tensorflow.compat.v1 as tf
tf.compat.v1.disable_eager_execution()
sess = tf.Session()
a = tf.constant(10)
b= tf.constant(12)
sess.run(a+b)
```

è¾“å‡ºç»“æœä¸º22ï¼Œå®‰è£…æˆåŠŸ

# ç¬¬1è®² ç¥ç»ç½‘ç»œè®¡ç®—

å…ˆçœ‹ä¸€ä¸ªç”¨ç¥ç»ç½‘ç»œå®ç°é¸¢å°¾èŠ±åˆ†ç±»çš„ä¾‹å­

![image-20210913171147836](img/image-20210913171147836.png)

æ ¹æ®ç”Ÿç‰©ç¥ç»å…ƒç®€åŒ–çš„MPæ¨¡å‹ï¼Œè¾“å…¥ç‰¹å¾ä¹˜ä»¥çº¿ä¸Šæƒé‡ï¼Œæ±‚å’Œï¼Œå†é€šè¿‡ä¸€ä¸ªéçº¿æ€§å‡½æ•°è¾“å‡º

![image-20210913170826064](img/image-20210913170826064.png)

å…ˆä¸çœ‹è¿™ä¸ªéçº¿æ€§å‡½æ•°ï¼Œè¿›ä¸€æ­¥ç®€åŒ–

![image-20210913170941221](img/image-20210913170941221.png)

å…·ä½“æ­¥éª¤ï¼š

1.æ­å»ºç½‘ç»œå¹¶éšæœºåˆå§‹åŒ–å‚æ•°ï¼Œè¿™é‡Œé‡Œé¢æ¯ä¸ªç¥ç»å…ƒy~0~ ã€y~1~ ã€y~2~ å’Œå‰é¢çš„æ¯ä¸€ä¸ªç»“ç‚¹x~0~ ã€x~1~ ã€x~2~ ã€x~3~ éƒ½æœ‰è¿æ¥å…³ç³»ï¼Œç§°è¿™æ ·çš„ç½‘ç»œç»“æ„ä¸º**å…¨è¿æ¥ç½‘ç»œ**

![image-20210913171711192](img/image-20210913171711192.png)

2.å–‚å¦‚è¾“å…¥ç‰¹å¾å’Œå¯¹åº”æ ‡ç­¾

![image-20210913171824704](img/image-20210913171824704.png)

3.å‰å‘ä¼ æ’­ï¼Œæ ¹æ®å…¬å¼ä»£å…¥æ•°æ®ï¼Œè®¡ç®—å‡ºy

![image-20210913172235603](img/image-20210913172235603.png)

4.æŸå¤±å‡½æ•°ï¼Œè¡¨ç¤ºå‰å‘ä¼ æ’­è®¡ç®—çš„yä¸æ ‡å‡†ç­”æ¡ˆä¹‹é—´çš„å·®è·

![image-20210913172458906](img/image-20210913172458906.png)

5.æ¢¯åº¦ä¸‹é™

![image-20210913172601426](img/image-20210913172601426.png)

6.åå‘ä¼ æ’­

![image-20210913172721306](img/image-20210913172721306.png)

## TensorFlow2.xåŸºæœ¬ä½¿ç”¨

### æ•°æ®ç±»å‹

![image-20210914112328584](img/image-20210914112328584.png)

![image-20210914112339424](img/image-20210914112339424.png)

### åˆ›å»ºTensor

#### 1. tf.constant

**tf.constant**(å¼ é‡å†…å®¹ï¼Œdtype=æ•°æ®ç±»å‹(å¯é€‰))

```python
import tensorflow as tf
a=tf.constant([1,5],dtype=tf.int64)
print(a)
print(a.dtype)
print(a.shape)
```

> è¾“å‡ºï¼š
>
> a: tf.Tensor([1 5], shape=(2,), dtype=int64)
> a.dtype: <dtype: 'int64'>
> a.shape: (2,)

#### 2. tf. convert_to_tensor

**tf. convert_to_tensor**(æ•°æ®åï¼Œdtype=æ•°æ®ç±»å‹(å¯é€‰)),å°†numpyçš„æ•°æ®ç±»å‹è½¬æ¢ä¸ºTensoræ•°æ®ç±»å‹

```python
import tensorflow as tf
import numpy as np

a = np.arange(0, 5)
b = tf.convert_to_tensor(a, dtype=tf.int64)
print("a:", a)
print("b:", b)
```

> è¾“å‡ºï¼š
>
> a: [0 1 2 3 4]
> b: tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)

#### 3. tf.zeros/ones/fill

![image-20210914113325039](img/image-20210914113325039.png)

```python
import tensorflow as tf

a = tf.zeros([2, 3])
b = tf.ones(4)
c = tf.fill([2, 2], 9)
print("a:", a)
print("b:", b)
print("c:", c)
```

> è¾“å‡ºï¼š
>
> a: tf.Tensor(
> [[0. 0. 0.]
>  [0. 0. 0.]], shape=(2, 3), dtype=float32)
> b: tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32)
> c: tf.Tensor(
> [[9 9]
>  [9 9]], shape=(2, 2), dtype=int32)

#### 4. tf.random

![image-20210914114401522](img/image-20210914114401522.png)

```python
import tensorflow as tf

d = tf.random.normal([2, 2], mean=0.5, stddev=1)
print("d:", d)
e = tf.random.truncated_normal([2, 2], mean=0.5, stddev=1)
print("e:", e)
```

> è¾“å‡ºï¼š
>
> d: tf.Tensor(
> [[ 2.0808887 -0.5846902]
>  [ 2.8404007  0.1462099]], shape=(2, 2), dtype=float32)
> e: tf.Tensor(
> [[ 1.8202503  -0.20955426]
>  [ 1.4057683   0.5466141 ]], shape=(2, 2), dtype=float32)



![image-20210914114851098](img/image-20210914114851098.png)

```python
import tensorflow as tf

f = tf.random.uniform([2, 2], minval=0, maxval=1)
print("f:", f)
```

> è¾“å‡ºï¼š
>
> f: tf.Tensor(
> [[0.3525442  0.39133584]
>  [0.63871145 0.6516905 ]], shape=(2, 2), dtype=float32)

### è½´çš„ç†è§£

**åœ¨äºŒç»´å¼ é‡ä¸­å¯ä»¥é€šè¿‡æŒ‡å®šaxisæŒ‡å®šè®¡ç®—è½´ï¼Œå¦‚æœä¸æŒ‡å®šè½´ï¼Œåˆ™æ‰€æœ‰å…ƒç´ å‚ä¸è®¡ç®—**

[**å…³äºè½´çš„ç†è§£**](https://www.cnblogs.com/monteyang/p/13091387.html)

ä¸€ä¸ªå¤šç»´åˆ—è¡¨,æœ€å¤–å±‚æ˜¯0è½´,æ ¹æ®ä¸åŒçš„å‡½æ•°,è®¡ç®—åè¯¥è½´çš„ç»´åº¦**ä¸Šå‡ï¼ˆå¦‚ç‹¬çƒ­ç¼–ç ï¼‰**ã€**ä¸‹é™ï¼ˆå¦‚æ±‚å’Œï¼‰**æˆ–**ä¸å˜**

### å¸¸ç”¨å‡½æ•°

#### 1. tf.castã€tf.reduce_min/max/mean/sum

![image-20210914142405487](img/image-20210914142405487.png)

```python
import tensorflow as tf

x1 = tf.constant([1., 2., 3.], dtype=tf.float64)
print("x1:", x1)
x2 = tf.cast(x1, tf.int32)
print("x2", x2)
print("minimum of x2ï¼š", tf.reduce_min(x2))
print("maxmum of x2:", tf.reduce_max(x2))
```

> è¾“å‡ºï¼š
>
> x1: tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64)
> x2 tf.Tensor([1 2 3], shape=(3,), dtype=int32)
> minimum of x2ï¼š tf.Tensor(1, shape=(), dtype=int32)
> maxmum of x2: tf.Tensor(3, shape=(), dtype=int32)

-----

![image-20210914143017028](img/image-20210914143017028.png)

```python
import tensorflow as tf

x = tf.constant([[1, 2, 3], [2, 2, 3]])
print("x:", x)
print("mean of x:", tf.reduce_mean(x))  # æ±‚xä¸­æ‰€æœ‰æ•°çš„å‡å€¼
print("sum of x:", tf.reduce_sum(x, axis=1))  # æ±‚æ¯ä¸€è¡Œçš„å’Œ
```

> è¾“å‡ºï¼š
>
> x: tf.Tensor(
> [[1 2 3]
>  [2 2 3]], shape=(2, 3), dtype=int32)
> mean of x: tf.Tensor(2, shape=(), dtype=int32)
> sum of x: tf.Tensor([6 7], shape=(2,), dtype=int32)

------

#### 2. tf.Variable

![image-20210914145340884](img/image-20210914145340884.png)

-----------------

#### 3. tf.add/subtract/multiply/divide

å½¢çŠ¶ç›¸åŒæ‰èƒ½å››åˆ™è¿ç®—

![image-20210914145429177](img/image-20210914145429177.png)

```python
import tensorflow as tf

a = tf.ones([1, 3])
b = tf.fill([1, 3], 3.)
print("a:", a)
print("b:", b)
print("a+b:", tf.add(a, b))
print("a-b:", tf.subtract(a, b))
print("a*b:", tf.multiply(a, b))
print("b/a:", tf.divide(b, a))
```

> è¾“å‡ºï¼š
>
> a: tf.Tensor([[1. 1. 1.]], shape=(1, 3), dtype=float32)
> b: tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)
> a+b: tf.Tensor([[4. 4. 4.]], shape=(1, 3), dtype=float32)
> a-b: tf.Tensor([[-2. -2. -2.]], shape=(1, 3), dtype=float32)
> a*b: tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)
> b/a: tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)

-----------

#### 4. tf.square/pow/sqrt

å¹³æ–¹ã€æ¬¡æ–¹ã€å¼€æ”¾æ“ä½œå¯¹æ‰€æœ‰å…ƒç´ è¿›è¡Œ

![image-20210914145942531](img/image-20210914145942531.png)

```python
import tensorflow as tf

a = tf.fill([1, 2], 3.)
print("a:", a)
print("açš„ç«‹æ–¹:", tf.pow(a, 3))
print("açš„å¹³æ–¹:", tf.square(a))
print("açš„å¼€æ–¹:", tf.sqrt(a))
```

> è¾“å‡ºï¼š
>
> a: tf.Tensor([[3. 3.]], shape=(1, 2), dtype=float32)
> açš„ç«‹æ–¹: tf.Tensor([[27. 27.]], shape=(1, 2), dtype=float32)
> açš„å¹³æ–¹: tf.Tensor([[9. 9.]], shape=(1, 2), dtype=float32)
> açš„å¼€æ–¹: tf.Tensor([[1.7320508 1.7320508]], shape=(1, 2), dtype=float32)

----

#### 5. tf.matmul

![image-20210914150354496](img/image-20210914150354496.png)

```python
import tensorflow as tf

a = tf.ones([3, 2])
b = tf.fill([2, 3], 3.)
print("a:", a)
print("b:", b)
print("a*b:", tf.matmul(a, b))
```

> è¾“å‡ºï¼š
>
> a: tf.Tensor(
> [[1. 1.]
>  [1. 1.]
>  [1. 1.]], shape=(3, 2), dtype=float32)
> b: tf.Tensor(
> [[3. 3. 3.]
>  [3. 3. 3.]], shape=(2, 3), dtype=float32)
> a*b: tf.Tensor(
> [[6. 6. 6.]
>  [6. 6. 6.]
>  [6. 6. 6.]], shape=(3, 3), dtype=float32)

--------

#### 6. tf.data.Dataset.from_tensor_slices

![image-20210914150632512](img/image-20210914150632512.png)

```python
import tensorflow as tf

features = tf.constant([12, 23, 10, 17])
labels = tf.constant([0, 1, 1, 0])
dataset = tf.data.Dataset.from_tensor_slices((features, labels))
for element in dataset:
    print(element)
```

> è¾“å‡ºï¼š
>
> (<tf.Tensor: shape=(), dtype=int32, numpy=12>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)
> (<tf.Tensor: shape=(), dtype=int32, numpy=23>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)
> (<tf.Tensor: shape=(), dtype=int32, numpy=10>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)
> (<tf.Tensor: shape=(), dtype=int32, numpy=17>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)

-------

#### 7. tf.GradientTape

```python
tf.GradientTape(
    persistent=False, watch_accessed_variables=True
)
```

```python
x = tf.constant(5.0)
with tf.GradientTape() as g:
  g.watch(x)	# ç›‘è§†å¯è®­ç»ƒå˜é‡x
  with tf.GradientTape() as gg:
    gg.watch(x)
    y = x * x
  dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x
d2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2
print(dy_dx)

print(d2y_dx2)
```

GradientTapeé»˜è®¤åœ¨è¿›è¡Œä¸€æ¬¡æ±‚å¯¼åå°±é‡Šæ”¾,å¦‚æœè¦å¤šæ¬¡æ±‚å¯¼è¦å°† `persistent`è®¾ä¸ºTrue,å¹¶æ‰‹åŠ¨é‡Šæ”¾`del g`

![image-20210914150925223](img/image-20210914150925223.png)

```python
import tensorflow as tf

with tf.GradientTape() as tape:
    x = tf.Variable(tf.constant(3.0))
    y = tf.pow(x, 2)
grad = tape.gradient(y, x)
print(grad)
```

> è¾“å‡ºï¼š
>
> tf.Tensor(6.0, shape=(), dtype=float32)

------

#### 8. enumerate

![image-20210914151135319](img/image-20210914151135319.png)

----

#### 9. tf.one_hot

**é»˜è®¤å¯¹æœ€å†…å±‚è®¡ç®—**,è®¡ç®—åç»´åº¦ä¸Šå‡

```python
tf.one_hot(
    indices,        #è¾“å…¥çš„tensorï¼Œåœ¨æ·±åº¦å­¦ä¹ ä¸­ä¸€èˆ¬æ˜¯ç»™å®šçš„labelsï¼Œé€šå¸¸æ˜¯æ•°å­—åˆ—è¡¨ï¼Œå±äºä¸€ç»´è¾“å…¥ï¼Œä¹Ÿå¯ä»¥æ˜¯å¤šç»´ã€‚
    depth,          #ä¸€ä¸ªæ ‡é‡ï¼Œç”¨äºå®šä¹‰ä¸€ä¸ª one hot ç»´åº¦çš„æ·±åº¦
    on_value=None,  #å®šä¹‰åœ¨ indices[j] = i æ—¶å¡«å……è¾“å‡ºçš„å€¼çš„æ ‡é‡ï¼Œé»˜è®¤ä¸º1
    off_value=None, #å®šä¹‰åœ¨ indices[j] != i æ—¶å¡«å……è¾“å‡ºçš„å€¼çš„æ ‡é‡ï¼Œé»˜è®¤ä¸º0
    axis=None,      #è¦å¡«å……çš„è½´ï¼Œé»˜è®¤ä¸º-1ï¼Œå³ä¸€ä¸ªæ–°çš„æœ€å†…å±‚è½´
    dtype=None,     
    name=None
)
```

![image-20210914151423785](img/image-20210914151423785.png)

å¯ç”¨ tf.one_hot(å¾…è½¬æ¢æ•°æ®ï¼Œdepth=å‡ åˆ†ç±»)å‡½æ•°å®ç°ç”¨ç‹¬çƒ­ç è¡¨ç¤ºæ ‡ç­¾ï¼Œ åœ¨åˆ†ç±»é—®é¢˜ä¸­å¾ˆå¸¸è§ã€‚æ ‡è®°ç±»åˆ«ä¸ºä¸º 1 å’Œ 0ï¼Œå…¶ä¸­ 1 è¡¨ç¤ºæ˜¯ï¼Œ0 è¡¨ç¤ºéã€‚å¦‚åœ¨é¸¢ å°¾èŠ±åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¦‚æœæ ‡ç­¾æ˜¯ 1ï¼Œè¡¨ç¤ºåˆ†ç±»ç»“æœæ˜¯ 1 æ‚è‰²é¸¢å°¾ï¼Œå…¶ç”¨æŠŠå®ƒç”¨ç‹¬çƒ­ ç è¡¨ç¤ºå°±æ˜¯ 0,1,0ï¼Œè¿™æ ·å¯ä»¥è¡¨ç¤ºå‡ºæ¯ä¸ªåˆ†ç±»çš„æ¦‚ç‡ï¼šä¹Ÿå°±æ˜¯ç™¾åˆ†ä¹‹ 0 çš„å¯èƒ½æ˜¯ 0 ç‹—å°¾è‰é¸¢å°¾ï¼Œç™¾åˆ†ç™¾çš„å¯èƒ½æ˜¯ 1 æ‚è‰²é¸¢å°¾ï¼Œç™¾åˆ†ä¹‹ 0 çš„å¯èƒ½æ˜¯å¼—å‰å°¼äºšé¸¢å°¾ã€‚

```python
import tensorflow as tf

classes = 3
labels = tf.constant([1, 0, 2])  # è¾“å…¥çš„å…ƒç´ å€¼æœ€å°ä¸º0ï¼Œæœ€å¤§ä¸º2
output = tf.one_hot(labels, depth=classes)
print("result of labels1:", output)
print("\n")
```

> è¾“å‡ºï¼š
>
> result of labels1: tf.Tensor(
> [[0. 1. 0.]
>  [1. 0. 0.]
>  [0. 0. 1.]], shape=(3, 3), dtype=float32)

---------

#### 10. tf.nn.softmax

**é»˜è®¤å¯¹æœ€å†…å±‚ç»´åº¦è®¡ç®—**,è®¡ç®—åç»´åº¦ä¸å˜

```python
tf.nn.softmax(
    logits,
    axis=None,
    name=None,
    dim=None
)
```

![image-20210914151924391](img/image-20210914151924391.png)

```python
import tensorflow as tf

x1 = tf.constant([[5.8, 4.0, 1.2, 0.2]])  # 5.8,4.0,1.2,0.2ï¼ˆ0ï¼‰
w1 = tf.constant([[-0.8, -0.34, -1.4],
                  [0.6, 1.3, 0.25],
                  [0.5, 1.45, 0.9],
                  [0.65, 0.7, -1.2]])
b1 = tf.constant([2.52, -3.1, 5.62])
y = tf.matmul(x1, w1) + b1
print("x1.shape:", x1.shape)
print("w1.shape:", w1.shape)
print("b1.shape:", b1.shape)
print("y.shape:", y.shape)
print("y:", y)

#####ä»¥ä¸‹ä»£ç å¯å°†è¾“å‡ºç»“æœyè½¬åŒ–ä¸ºæ¦‚ç‡å€¼#####
y_dim = tf.squeeze(y)  # å»æ‰yä¸­çº¬åº¦1ï¼ˆè§‚å¯Ÿy_dimä¸ y æ•ˆæœå¯¹æ¯”ï¼‰
y_pro = tf.nn.softmax(y_dim)  # ä½¿y_dimç¬¦åˆæ¦‚ç‡åˆ†å¸ƒï¼Œè¾“å‡ºä¸ºæ¦‚ç‡å€¼äº†
print("y_dim:", y_dim)
print("y_pro:", y_pro)
```

> è¾“å‡ºï¼š
>
> x1.shape: (1, 4)
> w1.shape: (4, 3)
> b1.shape: (3,)
> y.shape: (1, 3)
> y: tf.Tensor([[ 1.010945    2.0069656  -0.66328144]], shape=(1, 3), dtype=float32)
> y_dim: tf.Tensor([ 1.010945    2.0069656  -0.66328144], shape=(3,), dtype=float32)
> y_pro: tf.Tensor([0.2567434  0.6951293  0.04812736], shape=(3,), dtype=float32)

------

#### 11. (Tensor.)assign_sub

![image-20210914152725790](img/image-20210914152725790.png)

-------

#### 12. tf.argmax

![image-20210914152840734](img/image-20210914152840734.png)

```python
import numpy as np
import tensorflow as tf

test = np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]])
print("test:\n", test)
print("æ¯ä¸€åˆ—çš„æœ€å¤§å€¼çš„ç´¢å¼•ï¼š", tf.argmax(test, axis=0))  # è¿”å›æ¯ä¸€åˆ—æœ€å¤§å€¼çš„ç´¢å¼•
print("æ¯ä¸€è¡Œçš„æœ€å¤§å€¼çš„ç´¢å¼•", tf.argmax(test, axis=1))  # è¿”å›æ¯ä¸€è¡Œæœ€å¤§å€¼çš„ç´¢å¼•
```

> è¾“å‡ºï¼š
>
> æ¯ä¸€åˆ—çš„æœ€å¤§å€¼çš„ç´¢å¼•ï¼š tf.Tensor([3 3 1], shape=(3,), dtype=int64)
> æ¯ä¸€è¡Œçš„æœ€å¤§å€¼çš„ç´¢å¼• tf.Tensor([2 2 0 0], shape=(4,), dtype=int64)

## é¸¢å°¾èŠ±åˆ†ç±»å®ä¾‹

å…ˆå¯¼å…¥æ•°æ®é›†çœ‹ä¸€çœ‹

```python
from sklearn import datasets
from pandas import DataFrame
import pandas as pd

x_data = datasets.load_iris().data  # .dataè¿”å›irisæ•°æ®é›†æ‰€æœ‰è¾“å…¥ç‰¹å¾
y_data = datasets.load_iris().target  # .targetè¿”å›irisæ•°æ®é›†æ‰€æœ‰æ ‡ç­¾
print("x_data from datasets: \n", x_data)
print("y_data from datasets: \n", y_data)

x_data = DataFrame(x_data, columns=['èŠ±è¼é•¿åº¦', 'èŠ±è¼å®½åº¦', 'èŠ±ç“£é•¿åº¦', 'èŠ±ç“£å®½åº¦']) # ä¸ºè¡¨æ ¼å¢åŠ è¡Œç´¢å¼•ï¼ˆå·¦ä¾§ï¼‰å’Œåˆ—æ ‡ç­¾ï¼ˆä¸Šæ–¹ï¼‰
pd.set_option('display.unicode.east_asian_width', True)  # è®¾ç½®åˆ—åå¯¹é½
print("x_data add index: \n", x_data)

x_data['ç±»åˆ«'] = y_data  # æ–°åŠ ä¸€åˆ—ï¼Œåˆ—æ ‡ç­¾ä¸ºâ€˜ç±»åˆ«â€™ï¼Œæ•°æ®ä¸ºy_data
print("x_data add a column: \n", x_data)

#ç±»å‹ç»´åº¦ä¸ç¡®å®šæ—¶ï¼Œå»ºè®®ç”¨printå‡½æ•°æ‰“å°å‡ºæ¥ç¡®è®¤æ•ˆæœ
```

**å…·ä½“æ­¥éª¤ï¼š**

- å‡†å¤‡æ•°æ®
  - æ•°æ®é›†è¯»å…¥
  - æ•°æ®é›†ä¹±åº
  - ç”Ÿæˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆå³ x_train / y_trainï¼›x_test / y_testï¼‰
  - é…æˆï¼ˆè¾“å…¥ç‰¹å¾ï¼Œæ ‡ç­¾ï¼‰å¯¹ï¼Œæ¯æ¬¡è¯»å…¥ä¸€å°æ’®ï¼ˆbatchï¼‰

- æ­å»ºç½‘ç»œ
  - å®šä¹‰ç¥ç»ç½‘è·¯ä¸­æ‰€æœ‰å¯è®­ç»ƒå‚æ•°

- å‚æ•°ä¼˜åŒ–
  - åµŒå¥—å¾ªç¯è¿­ä»£ï¼Œwithç»“æ„æ›´æ–°å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰loss
- æµ‹è¯•æ•ˆæœ
  - è®¡ç®—å½“å‰å‚æ•°å‰å‘ä¼ æ’­åçš„å‡†ç¡®ç‡ï¼Œæ˜¾ç¤ºå½“å‰acc
- acc / losså¯è§†åŒ–

1.æ•°æ®é›†è¯»å…¥

```python
# å¯¼å…¥æ‰€éœ€æ¨¡å—
import tensorflow as tf
from sklearn import datasets
from matplotlib import pyplot as plt
import numpy as np

# å¯¼å…¥æ•°æ®ï¼Œåˆ†åˆ«ä¸ºè¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾
x_data = datasets.load_iris().data
y_data = datasets.load_iris().target
```

2.æ•°æ®é›†ä¹±åº

```python
# éšæœºæ‰“ä¹±æ•°æ®ï¼ˆå› ä¸ºåŸå§‹æ•°æ®æ˜¯é¡ºåºçš„ï¼Œé¡ºåºä¸æ‰“ä¹±ä¼šå½±å“å‡†ç¡®ç‡ï¼‰
# seed: éšæœºæ•°ç§å­ï¼Œæ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œå½“è®¾ç½®ä¹‹åï¼Œæ¯æ¬¡ç”Ÿæˆçš„éšæœºæ•°éƒ½ä¸€æ ·ï¼ˆä¸ºæ–¹ä¾¿æ•™å­¦ï¼Œä»¥ä¿æ¯ä½åŒå­¦ç»“æœä¸€è‡´ï¼‰
np.random.seed(116)  # ä½¿ç”¨ç›¸åŒçš„seedï¼Œä¿è¯è¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾ä¸€ä¸€å¯¹åº”
np.random.shuffle(x_data)   # ç”Ÿæˆéšæœºåˆ—è¡¨ï¼ˆæ‰“ä¹±é¡ºåºï¼‰
np.random.seed(116)
np.random.shuffle(y_data)
tf.random.set_seed(116)
```

3.å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†

```python
# å°†æ‰“ä¹±åçš„æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œè®­ç»ƒé›†ä¸ºå‰120è¡Œï¼Œæµ‹è¯•é›†ä¸ºå30è¡Œ
x_train = x_data[:-30]
y_train = y_data[:-30]
x_test = x_data[-30:]
y_test = y_data[-30:]
```

```python
# è½¬æ¢xçš„æ•°æ®ç±»å‹ï¼Œå¦åˆ™åé¢çŸ©é˜µç›¸ä¹˜æ—¶ä¼šå› æ•°æ®ç±»å‹ä¸ä¸€è‡´æŠ¥é”™
x_train = tf.cast(x_train, tf.float32)
x_test = tf.cast(x_test, tf.float32)
```

4.å°†æ•°æ®é…æˆç‰¹å¾--æ ‡ç­¾å¯¹

```python
# from_tensor_sliceså‡½æ•°ä½¿è¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾å€¼ä¸€ä¸€å¯¹åº”ã€‚ï¼ˆæŠŠæ•°æ®é›†åˆ†æ‰¹æ¬¡ï¼Œæ¯ä¸ªæ‰¹æ¬¡batchç»„æ•°æ®ï¼‰
train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)
```

5.æ­å»ºç½‘ç»œ

```python
# ç”Ÿæˆç¥ç»ç½‘ç»œçš„å‚æ•°ï¼Œ4ä¸ªè¾“å…¥ç‰¹å¾æ•…ï¼Œè¾“å…¥å±‚ä¸º4ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼›å› ä¸º3åˆ†ç±»ï¼Œæ•…è¾“å‡ºå±‚ä¸º3ä¸ªç¥ç»å…ƒï¼ˆæ²¡æœ‰éšè—å±‚ï¼‰
# ç”¨tf.Variable()æ ‡è®°å‚æ•°å¯è®­ç»ƒ
# ä½¿ç”¨seedä½¿æ¯æ¬¡ç”Ÿæˆçš„éšæœºæ•°ç›¸åŒï¼ˆæ–¹ä¾¿æ•™å­¦ï¼Œä½¿å¤§å®¶ç»“æœéƒ½ä¸€è‡´ï¼Œåœ¨ç°å®ä½¿ç”¨æ—¶ä¸å†™seedï¼‰
w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))
b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))
```

```python
lr = 0.1  # å­¦ä¹ ç‡ä¸º0.1
train_loss_results = []  # å°†æ¯è½®çš„lossè®°å½•åœ¨æ­¤åˆ—è¡¨ä¸­ï¼Œä¸ºåç»­ç”»lossæ›²çº¿æä¾›æ•°æ®
test_acc = []  # å°†æ¯è½®çš„accè®°å½•åœ¨æ­¤åˆ—è¡¨ä¸­ï¼Œä¸ºåç»­ç”»accæ›²çº¿æä¾›æ•°æ®
epoch = 500  # å¾ªç¯500è½®
loss_all = 0  # æ¯è½®åˆ†4ä¸ªstepï¼Œloss_allè®°å½•å››ä¸ªstepç”Ÿæˆçš„4ä¸ªlossçš„å’Œ
```

6.ä¼˜åŒ–ã€æµ‹è¯•

```python
# è®­ç»ƒéƒ¨åˆ†
for epoch in range(epoch):  #æ•°æ®é›†çº§åˆ«çš„å¾ªç¯ï¼Œæ¯ä¸ªepochå¾ªç¯ä¸€æ¬¡æ•°æ®é›†
    for step, (x_train, y_train) in enumerate(train_db):  #batchçº§åˆ«çš„å¾ªç¯ ï¼Œæ¯ä¸ªstepå¾ªç¯ä¸€ä¸ªbatch
        with tf.GradientTape() as tape:  # withç»“æ„è®°å½•æ¢¯åº¦ä¿¡æ¯
            y = tf.matmul(x_train, w1) + b1  # ç¥ç»ç½‘ç»œä¹˜åŠ è¿ç®—
            y = tf.nn.softmax(y)  # ä½¿è¾“å‡ºyç¬¦åˆæ¦‚ç‡åˆ†å¸ƒï¼ˆæ­¤æ“ä½œåä¸ç‹¬çƒ­ç åŒé‡çº§ï¼Œå¯ç›¸å‡æ±‚lossï¼‰
            y_ = tf.one_hot(y_train, depth=3)  # å°†æ ‡ç­¾å€¼è½¬æ¢ä¸ºç‹¬çƒ­ç æ ¼å¼ï¼Œæ–¹ä¾¿è®¡ç®—losså’Œaccuracy
            loss = tf.reduce_mean(tf.square(y_ - y))  # é‡‡ç”¨å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°mse = mean(sum(y-out)^2)
            loss_all += loss.numpy()  # å°†æ¯ä¸ªstepè®¡ç®—å‡ºçš„lossç´¯åŠ ï¼Œä¸ºåç»­æ±‚losså¹³å‡å€¼æä¾›æ•°æ®ï¼Œè¿™æ ·è®¡ç®—çš„lossæ›´å‡†ç¡®
        # è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
        grads = tape.gradient(loss, [w1, b1])

        # å®ç°æ¢¯åº¦æ›´æ–° w1 = w1 - lr * w1_grad    b = b - lr * b_grad
        w1.assign_sub(lr * grads[0])  # å‚æ•°w1è‡ªæ›´æ–°
        b1.assign_sub(lr * grads[1])  # å‚æ•°bè‡ªæ›´æ–°

    # æ¯ä¸ªepochï¼Œæ‰“å°lossä¿¡æ¯
    print("Epoch {}, loss: {}".format(epoch, loss_all/4))
    train_loss_results.append(loss_all / 4)  # å°†4ä¸ªstepçš„lossæ±‚å¹³å‡è®°å½•åœ¨æ­¤å˜é‡ä¸­
    loss_all = 0  # loss_allå½’é›¶ï¼Œä¸ºè®°å½•ä¸‹ä¸€ä¸ªepochçš„lossåšå‡†å¤‡

    # æµ‹è¯•éƒ¨åˆ†
    # total_correctä¸ºé¢„æµ‹å¯¹çš„æ ·æœ¬ä¸ªæ•°, total_numberä¸ºæµ‹è¯•çš„æ€»æ ·æœ¬æ•°ï¼Œå°†è¿™ä¸¤ä¸ªå˜é‡éƒ½åˆå§‹åŒ–ä¸º0
    total_correct, total_number = 0, 0
    for x_test, y_test in test_db:
        # ä½¿ç”¨æ›´æ–°åçš„å‚æ•°è¿›è¡Œé¢„æµ‹
        y = tf.matmul(x_test, w1) + b1
        y = tf.nn.softmax(y)
        pred = tf.argmax(y, axis=1)  # è¿”å›yä¸­æœ€å¤§å€¼çš„ç´¢å¼•ï¼Œå³é¢„æµ‹çš„åˆ†ç±»
        # å°†predè½¬æ¢ä¸ºy_testçš„æ•°æ®ç±»å‹
        pred = tf.cast(pred, dtype=y_test.dtype)
        # è‹¥åˆ†ç±»æ­£ç¡®ï¼Œåˆ™correct=1ï¼Œå¦åˆ™ä¸º0ï¼Œå°†boolå‹çš„ç»“æœè½¬æ¢ä¸ºintå‹
        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)
        # å°†æ¯ä¸ªbatchçš„correctæ•°åŠ èµ·æ¥
        correct = tf.reduce_sum(correct)
        # å°†æ‰€æœ‰batchä¸­çš„correctæ•°åŠ èµ·æ¥
        total_correct += int(correct)
        # total_numberä¸ºæµ‹è¯•çš„æ€»æ ·æœ¬æ•°ï¼Œä¹Ÿå°±æ˜¯x_testçš„è¡Œæ•°ï¼Œshape[0]è¿”å›å˜é‡çš„è¡Œæ•°
        total_number += x_test.shape[0]
    # æ€»çš„å‡†ç¡®ç‡ç­‰äºtotal_correct/total_number
    acc = total_correct / total_number
    test_acc.append(acc)
    print("Test_acc:", acc)
    print("--------------------------")
```

7.ç»˜åˆ¶æŸå¤±å‡½æ•°æ›²çº¿ä¸ç²¾åº¦æ›²çº¿

```python
# ç»˜åˆ¶ loss æ›²çº¿
plt.title('Loss Function Curve')  # å›¾ç‰‡æ ‡é¢˜
plt.xlabel('Epoch')  # xè½´å˜é‡åç§°
plt.ylabel('Loss')  # yè½´å˜é‡åç§°
plt.plot(train_loss_results, label="$Loss$")  # é€ç‚¹ç”»å‡ºtrian_loss_resultså€¼å¹¶è¿çº¿ï¼Œè¿çº¿å›¾æ ‡æ˜¯Loss
plt.legend()  # ç”»å‡ºæ›²çº¿å›¾æ ‡
plt.show()  # ç”»å‡ºå›¾åƒ

# ç»˜åˆ¶ Accuracy æ›²çº¿
plt.title('Acc Curve')  # å›¾ç‰‡æ ‡é¢˜
plt.xlabel('Epoch')  # xè½´å˜é‡åç§°
plt.ylabel('Acc')  # yè½´å˜é‡åç§°
plt.plot(test_acc, label="$Accuracy$")  # é€ç‚¹ç”»å‡ºtest_accå€¼å¹¶è¿çº¿ï¼Œè¿çº¿å›¾æ ‡æ˜¯Accuracy
plt.legend()
plt.show()
```

![image-20210914171023615](img/image-20210914171023615.png)

![image-20210914171107529](img/image-20210914171107529.png)

--------

# ç¬¬2è®² ç¥ç»ç½‘ç»œä¼˜åŒ–

## å¸¸ç”¨å‡½æ•°

### 1. ft.where

![image-20210914184839226](img/image-20210914184839226.png)

```python
import tensorflow as tf

a = tf.constant([1, 2, 3, 1, 1])
b = tf.constant([0, 1, 3, 4, 5])
c = tf.where(tf.greater(a, b), a, b)  # è‹¥a>bï¼ˆå¯¹åº”å…ƒç´ æ¯”è¾ƒï¼‰ï¼Œè¿”å›aå¯¹åº”ä½ç½®çš„å…ƒç´ ï¼Œå¦åˆ™è¿”å›bå¯¹åº”ä½ç½®çš„å…ƒç´ 
print("cï¼š", c)
```

> è¾“å‡ºï¼š
>
> cï¼š tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)

------

### 2. np.random.RandomState.rand()

![image-20210914185226693](img/image-20210914185226693.png)

```python
import numpy as np

rdm = np.random.RandomState(seed=1)
a = rdm.rand()
b = rdm.rand(2, 3)
print("a:", a)
print("b:", b)
```

> è¾“å‡ºï¼š
>
> a: 0.417022004702574
> b: [[7.20324493e-01 1.14374817e-04 3.02332573e-01]
>  [1.46755891e-01 9.23385948e-02 1.86260211e-01]]

------

### 3. np.vstack()

![image-20210914185347344](img/image-20210914185347344.png)

```python
import numpy as np

a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
c = np.vstack((a, b))
print("c:\n", c)
```

> è¾“å‡ºï¼š
>
> c:
>  [[1 2 3]
>  [4 5 6]]

---

### 4. np.mgrid[]/.ravel()/c_[]

![image-20210914185513153](img/image-20210914185513153.png)

```python
import numpy as np

# mgrid()è¿”å›è‹¥å¹²ç»„ç»´åº¦ç›¸åŒçš„ç­‰å·®æ•°ç»„ï¼Œå·¦é—­å³å¼€åŒºé—´
# 1:3:1å†³å®šäº†ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯2ï¼Œ2:4:0.5å†³å®šäº†ç¬¬äºŒä¸ªç»´åº¦æ˜¯4
x, y = np.mgrid[1:3:1, 2:4:0.5]
# å°†x, yæ‹‰ç›´ï¼Œå¹¶åˆå¹¶é…å¯¹ä¸ºäºŒç»´å¼ é‡ï¼Œç”ŸæˆäºŒç»´åæ ‡ç‚¹
grid = np.c_[x.ravel(), y.ravel()]
print("x:\n", x)
print("y:\n", y)
print("x.ravel():\n", x.ravel())
print("y.ravel():\n", y.ravel())
print('grid:\n', grid)
```

> è¾“å‡ºï¼š
>
> x:
>  [[1. 1. 1. 1.]
>  [2. 2. 2. 2.]]
> y:
>  [[2.  2.5 3.  3.5]
>  [2.  2.5 3.  3.5]]
> x.ravel():
>  [1. 1. 1. 1. 2. 2. 2. 2.]
> y.ravel():
>  [2.  2.5 3.  3.5 2.  2.5 3.  3.5]
> grid:
>  [[1.  2. ]
>  [1.  2.5]
>  [1.  3. ]
>  [1.  3.5]
>  [2.  2. ]
>  [2.  2.5]
>  [2.  3. ]
>  [2.  3.5]]

## ç¥ç»ç½‘ç»œå¤æ‚åº¦

![image-20210914191845264](img/image-20210914191845264.png)

## å­¦ä¹ ç‡

å­¦ä¹ ç‡å†³å®šäº†å‚æ•°æ›´æ–°çš„å¿«æ…¢

![image-20210914191948572](img/image-20210914191948572.png)

æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡

![image-20210914192248624](img/image-20210914192248624.png)

## æ¿€æ´»å‡½æ•°

æ¿€æ´»å‡½æ•°æ˜¯ç”¨æ¥åŠ å…¥éçº¿æ€§å› ç´ çš„ï¼Œå› ä¸ºçº¿æ€§æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ä¸å¤Ÿã€‚å¼•å…¥éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œå¯ä½¿æ·± å±‚ç¥ç»ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›æ›´åŠ å¼ºå¤§ã€‚
ä¼˜ç§€çš„æ¿€æ´»å‡½æ•°åº”æ»¡è¶³ï¼š 

- éçº¿æ€§ï¼š æ¿€æ´»å‡½æ•°éçº¿æ€§æ—¶ï¼Œå¤šå±‚ç¥ç»ç½‘ç»œå¯é€¼è¿‘æ‰€æœ‰å‡½æ•°
- å¯å¾®æ€§ï¼š ä¼˜åŒ–å™¨å¤§å¤šç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•° 
- å•è°ƒæ€§ï¼š å½“æ¿€æ´»å‡½æ•°æ˜¯å•è°ƒçš„ï¼Œèƒ½ä¿è¯å•å±‚ç½‘ç»œçš„æŸå¤±å‡½æ•°æ˜¯å‡¸å‡½æ•° 
- è¿‘ä¼¼æ’ç­‰æ€§ï¼š$f(x)â‰ˆx$ï¼Œå½“å‚æ•°åˆå§‹åŒ–ä¸ºéšæœºå°å€¼æ—¶ï¼Œç¥ç»ç½‘ç»œæ›´ç¨³å®š

æ¿€æ´»å‡½æ•°è¾“å‡ºå€¼çš„èŒƒå›´ï¼š

- æ¿€æ´»å‡½æ•°è¾“å‡ºä¸º**æœ‰é™å€¼**æ—¶ï¼ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•æ›´ç¨³å®š
- æ¿€æ´»å‡½æ•°è¾“å‡ºä¸º**æ— é™å€¼**æ—¶ï¼Œå»ºè®®**è°ƒå°å­¦ä¹ ç‡**

### sigmoidå‡½æ•°

![image-20210914195041567](img/image-20210914195041567.png)

ä»Šå¹´ç¥ç»ç½‘ç»œä½¿ç”¨$sigmoid$å‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°å·²ç»è¾ƒå°‘ï¼Œå› ä¸ºç¥ç»ç½‘ç»œæ›´æ–°å‚æ•°æ—¶éœ€è¦ä»è¾“å…¥å±‚åˆ°è¾“å‡ºå±‚è¿›è¡Œ**é“¾å¼æ±‚å¯¼**ï¼Œ$sigmoid$å‡½æ•°å¯¼æ•°çš„èŒƒå›´æ˜¯0-0.25ï¼Œé“¾å¼æ±‚å¯¼éœ€è¦å¤šå±‚å¯¼æ•°è¿ç»­ç›¸ä¹˜ï¼Œä¼šå‡ºç°å¤šä¸ª0-0.25ä¹‹é—´çš„è¿ç»­ç›¸ä¹˜ï¼Œç»“æœå°†è¶‹äº0ï¼Œå‚æ•°æ— æ³•æ›´æ–°ã€‚

æˆ‘ä»¬å¸Œæœ›è¾“å…¥æ¯å±‚ç¥ç»ç½‘ç»œçš„ç‰¹å¾æ˜¯ä»¥0ä¸ºå‡å€¼çš„å°æ•°å€¼ï¼Œä½†æ˜¯$sigmoid$å‡½æ•°çš„è¾“å‡ºéƒ½æ˜¯0-1ä¹‹é—´çš„æ­£æ•°ï¼Œä¼šä½¿æ”¶æ•›å˜æ…¢

**ä¼˜ç‚¹ï¼š**

- è¾“å‡ºæ˜ å°„åœ¨(0,1)ä¹‹é—´ï¼Œå•è°ƒè¿ç»­ï¼Œè¾“å‡ºèŒƒå›´æœ‰é™ï¼Œä¼˜åŒ–ç¨³å®šï¼Œå¯ç”¨ä½œè¾“å‡ºå±‚ï¼› 
- æ±‚å¯¼å®¹æ˜“ã€‚

**ç¼ºç‚¹ï¼š**

- æ˜“é€ æˆæ¢¯åº¦æ¶ˆå¤±ï¼›
- å‡ºé0å‡å€¼ï¼Œæ”¶æ•›æ…¢ï¼›
- å¹‚è¿ç®—å¤æ‚ï¼Œè®­ç»ƒæ—¶é—´é•¿ã€‚

sigmoidå‡½æ•°å¯åº”ç”¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚ç„¶è€Œï¼Œå½“å¤„ç†åˆ†ç±»é—®é¢˜ä½œå‡ºè¾“å‡ºæ—¶ï¼Œsigmoidå´æ— èƒ½ä¸ºåŠ›ã€‚ç®€å•åœ°è¯´ï¼Œsigmoidå‡½æ•°åªèƒ½å¤„ç†ä¸¤ä¸ªç±»ï¼Œä¸é€‚ç”¨äºå¤šåˆ†ç±»é—®é¢˜ã€‚è€Œsoftmaxå¯ä»¥æœ‰æ•ˆè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶ ä¸”softmaxå‡½æ•°å¤§éƒ½è¿ç”¨åœ¨ç¥ç»ç½‘è·¯ä¸­çš„æœ€åä¸€å±‚ç½‘ç»œä¸­ï¼Œä½¿å¾—å€¼å¾—åŒºé—´åœ¨ï¼ˆ0,1ï¼‰ä¹‹é—´ï¼Œè€Œä¸æ˜¯äºŒåˆ†ç±»çš„ã€‚

### Tanhå‡½æ•°

![image-20210914201811308](img/image-20210914201811308.png)

**ä¼˜ç‚¹ï¼š**

- æ¯”sigmoidå‡½æ•°æ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚
- ç›¸æ¯”sigmoidå‡½æ•°ï¼Œå…¶è¾“å‡ºä»¥0ä¸ºä¸­å¿ƒã€‚

**ç¼ºç‚¹ï¼š**

- æ˜“é€ æˆæ¢¯åº¦æ¶ˆå¤±ï¼›
- å¹‚è¿ç®—å¤æ‚ï¼Œè®­ç»ƒæ—¶é—´é•¿ã€‚

### Reluå‡½æ•°

![image-20210914202005247](img/image-20210914202005247.png)

**ä¼˜ç‚¹ï¼š**

- è§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜(åœ¨æ­£åŒºé—´)ï¼›

- åªéœ€åˆ¤æ–­è¾“å…¥æ˜¯å¦å¤§äº0ï¼Œè®¡ç®—é€Ÿåº¦å¿«ï¼›

- æ”¶æ•›é€Ÿåº¦è¿œå¿«äºsigmoidå’Œtanhï¼Œå› ä¸ºsigmoidå’Œtanhæ¶‰åŠå¾ˆå¤šæ±‚å¹‚çš„æ“ä½œï¼›

- æä¾›äº†ç¥ç»ç½‘ç»œçš„ç¨€ç–è¡¨è¾¾èƒ½åŠ›ã€‚

**ç¼ºç‚¹ï¼š**

- è¾“å‡ºé0å‡å€¼ï¼Œæ”¶æ•›æ…¢ï¼› 
- Dead ReLUé—®é¢˜ï¼šé€å…¥æ¿€æ´»å‡½æ•°çš„ç‰¹å¾æ˜¯è´Ÿæ•°æ—¶æ¿€æ´»å‡½æ•°æ˜¯0ï¼Œåå‘ä¼ æ’­å¾—åˆ°çš„æ¢¯åº¦æ˜¯0,å¯¼è‡´å‚æ•°æ— æ³•æ›´æ–°,é€ æˆç¥ç»å…ƒæ­»äº¡.é€ æˆç¥ç»å…ƒæ­»äº¡çš„æ ¹æœ¬åŸå› æ˜¯é€å…¥æ¿€æ´»å‡½æ•°çš„è´Ÿæ•°ç‰¹å¾å¤ªå¤šå¯¼è‡´,å¯ä»¥æ”¹è¿›éšæœºåˆå§‹åŒ–,**é¿å…è¿‡å¤šè´Ÿæ•°ç‰¹å¾é€å…¥reluå‡½æ•°**.**é€šè¿‡è®¾ç½®æ›´å°çš„å­¦ä¹ ç‡é¿å…è®­ç»ƒä¸­è¿‡å¤šè´Ÿæ•°ç‰¹å¾è¿›å…¥reluå‡½æ•°.**

### Leaky ReLU

![image-20210914202920224](img/image-20210914202920224.png)

ç†è®ºä¸Šæ¥è®²ï¼ŒLeaky ReLUæœ‰ReLUçš„æ‰€æœ‰ä¼˜ç‚¹ï¼Œå¤–åŠ ä¸ä¼šæœ‰Dead ReLUé—®é¢˜ï¼Œä½†æ˜¯åœ¨å®é™…æ“ä½œå½“ ä¸­ï¼Œå¹¶æ²¡æœ‰å®Œå…¨è¯æ˜Leaky ReLUæ€»æ˜¯å¥½äºReLUã€‚

### å»ºè®®

- **é¦–é€‰ReLUæ¿€æ´»å‡½æ•°**ï¼› 

- **å­¦ä¹ ç‡è®¾ç½®è¾ƒå°å€¼**ï¼› 

- **è¾“å…¥ç‰¹å¾æ ‡å‡†åŒ–**ï¼Œå³è®©è¾“å…¥ç‰¹å¾æ»¡è¶³ä»¥0ä¸ºå‡å€¼ï¼Œ1ä¸ºæ ‡å‡†å·®çš„æ­£æ€åˆ†å¸ƒï¼›
- **åˆå§‹å‚æ•°ä¸­å¿ƒåŒ–**ï¼Œå³è®©éšæœºç”Ÿæˆçš„å‚æ•°æ»¡è¶³ä»¥0ä¸ºå‡å€¼ï¼Œ$\sqrt{2/å½“å‰å±‚è¾“å…¥ç‰¹å¾æ•°}$ä¸ºæ ‡å‡†å·®æ­£æ€åˆ†å¸ƒã€‚

## æŸå¤±å‡½æ•°

losså‡½æ•°è¡¨å¾é¢„æµ‹å€¼yä¸æ ‡å‡†å€¼y_çš„å·®è·

### å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°MSE

$\operatorname{MSE}\left(\mathrm{y}_{-}, \mathrm{y}\right)=\frac{\sum_{i=1}^{n}\left(y-y_{-}\right)^{2}}{n}$

$loss\_mse = tf.reduce\_mean(tf.square(y\_ - y))$

### äº¤å‰ç†µæŸå¤±å‡½æ•°CE

CE(Cross Entropy)è¡¨å¾äº†ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»

$H(y\_ , y) = âˆ’âˆ‘ ğ‘¦\_ âˆ— ğ‘™nğ‘¦$

$tf.losses.categorical\_crossentropy(y\_ï¼Œy)$

å¯¹äºå¤šåˆ†ç±»é—®é¢˜ï¼Œç¥ç»ç½‘ç»œçš„è¾“å‡ºä¸€èˆ¬ä¸æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œå› æ­¤éœ€è¦å¼•å…¥softmaxå±‚ï¼Œä½¿å¾—è¾“å‡ºæœä»æ¦‚ç‡åˆ†å¸ƒã€‚

![image-20210915104319268](img/image-20210915104319268.png)

```python
# softmaxä¸äº¤å‰ç†µæŸå¤±å‡½æ•°çš„ç»“åˆ
import tensorflow as tf
import numpy as np

y_ = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]])
y = np.array([[12, 3, 2], [3, 10, 1], [1, 2, 5], [4, 6.5, 1.2], [3, 6, 1]])
y_pro = tf.nn.softmax(y)
loss_ce1 = tf.losses.categorical_crossentropy(y_,y_pro)
loss_ce2 = tf.nn.softmax_cross_entropy_with_logits(y_, y)

print('åˆ†æ­¥è®¡ç®—çš„ç»“æœ:\n', loss_ce1)
print('ç»“åˆè®¡ç®—çš„ç»“æœ:\n', loss_ce2)

# è¾“å‡ºçš„ç»“æœç›¸åŒ
```

> è¾“å‡º:
>
> åˆ†æ­¥è®¡ç®—çš„ç»“æœ:
>  tf.Tensor(
> [1.68795487e-04 1.03475622e-03 6.58839038e-02 2.58349207e+00
>  5.49852354e-02], shape=(5,), dtype=float64)
> ç»“åˆè®¡ç®—çš„ç»“æœ:
>  tf.Tensor(
> [1.68795487e-04 1.03475622e-03 6.58839038e-02 2.58349207e+00
>  5.49852354e-02], shape=(5,), dtype=float64)

### è‡ªå®šä¹‰æŸå¤±å‡½æ•°

![image-20210915105843169](img/image-20210915105843169.png)

## æ¬ æ‹Ÿåˆä¸è¿‡æ‹Ÿåˆ

æ¬ æ‹Ÿåˆçš„è§£å†³æ–¹æ³•ï¼š

- å¢åŠ è¾“å…¥ç‰¹å¾é¡¹
- å¢åŠ ç½‘ç»œå‚æ•°

- å‡å°‘æ­£åˆ™åŒ–å‚æ•°

è¿‡æ‹Ÿåˆçš„è§£å†³æ–¹æ³•ï¼š

- æ•°æ®æ¸…æ´—

- å¢å¤§è®­ç»ƒé›†

- é‡‡ç”¨æ­£åˆ™åŒ–

- å¢å¤§æ­£åˆ™åŒ–å‚æ•°

## æ­£åˆ™åŒ–

![image-20210915193017199](img/image-20210915193017199.png)

[æœºå™¨å­¦ä¹ ä¸­æ­£åˆ™åŒ–é¡¹L1å’ŒL2çš„ç›´è§‚ç†è§£](https://blog.csdn.net/jinping_shi/article/details/52433975)

- L1æ­£åˆ™åŒ–å¯ä»¥äº§ç”Ÿç¨€ç–æƒå€¼çŸ©é˜µï¼Œå³äº§ç”Ÿä¸€ä¸ªç¨€ç–æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºç‰¹å¾é€‰æ‹©
- L2æ­£åˆ™åŒ–å¯ä»¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰ï¼›ä¸€å®šç¨‹åº¦ä¸Šï¼ŒL1ä¹Ÿå¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ

```python
# å¯¼å…¥æ‰€éœ€æ¨¡å—
import tensorflow as tf
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd

# è¯»å…¥æ•°æ®/æ ‡ç­¾ ç”Ÿæˆx_train y_train
df = pd.read_csv('dot.csv')
x_data = np.array(df[['x1', 'x2']])
y_data = np.array(df['y_c'])

x_train = x_data
y_train = y_data.reshape(-1, 1)

Y_c = [['red' if y else 'blue'] for y in y_train]

# è½¬æ¢xçš„æ•°æ®ç±»å‹ï¼Œå¦åˆ™åé¢çŸ©é˜µç›¸ä¹˜æ—¶ä¼šå› æ•°æ®ç±»å‹é—®é¢˜æŠ¥é”™
x_train = tf.cast(x_train, tf.float32)
y_train = tf.cast(y_train, tf.float32)

# from_tensor_sliceså‡½æ•°åˆ‡åˆ†ä¼ å…¥çš„å¼ é‡çš„ç¬¬ä¸€ä¸ªç»´åº¦ï¼Œç”Ÿæˆç›¸åº”çš„æ•°æ®é›†ï¼Œä½¿è¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾å€¼ä¸€ä¸€å¯¹åº”
train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)

# ç”Ÿæˆç¥ç»ç½‘ç»œçš„å‚æ•°ï¼Œè¾“å…¥å±‚ä¸º4ä¸ªç¥ç»å…ƒï¼Œéšè—å±‚ä¸º32ä¸ªç¥ç»å…ƒï¼Œ2å±‚éšè—å±‚ï¼Œè¾“å‡ºå±‚ä¸º3ä¸ªç¥ç»å…ƒ
# ç”¨tf.Variable()ä¿è¯å‚æ•°å¯è®­ç»ƒ
w1 = tf.Variable(tf.random.normal([2, 11]), dtype=tf.float32)
b1 = tf.Variable(tf.constant(0.01, shape=[11]))

w2 = tf.Variable(tf.random.normal([11, 1]), dtype=tf.float32)
b2 = tf.Variable(tf.constant(0.01, shape=[1]))

lr = 0.01  # å­¦ä¹ ç‡ä¸º
epoch = 400  # å¾ªç¯è½®æ•°

# è®­ç»ƒéƒ¨åˆ†
for epoch in range(epoch):
    for step, (x_train, y_train) in enumerate(train_db):
        with tf.GradientTape() as tape:  # è®°å½•æ¢¯åº¦ä¿¡æ¯

            h1 = tf.matmul(x_train, w1) + b1  # è®°å½•ç¥ç»ç½‘ç»œä¹˜åŠ è¿ç®—
            h1 = tf.nn.relu(h1)
            y = tf.matmul(h1, w2) + b2

            # é‡‡ç”¨å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°mse = mean(sum(y-out)^2)
            loss_mse = tf.reduce_mean(tf.square(y_train - y))
            # æ·»åŠ l2æ­£åˆ™åŒ–
            loss_regularization = []
            # tf.nn.l2_loss(w)=sum(w ** 2) / 2
            loss_regularization.append(tf.nn.l2_loss(w1))
            loss_regularization.append(tf.nn.l2_loss(w2))
            # æ±‚å’Œ
            # ä¾‹ï¼šx=tf.constant(([1,1,1],[1,1,1]))
            #   tf.reduce_sum(x)
            # >>>6
            # loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))
            loss_regularization = tf.reduce_sum(loss_regularization)
            loss = loss_mse + 0.03 * loss_regularization #REGULARIZER = 0.03

        # è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
        variables = [w1, b1, w2, b2]
        grads = tape.gradient(loss, variables)

        # å®ç°æ¢¯åº¦æ›´æ–°
        # w1 = w1 - lr * w1_grad
        w1.assign_sub(lr * grads[0])
        b1.assign_sub(lr * grads[1])
        w2.assign_sub(lr * grads[2])
        b2.assign_sub(lr * grads[3])

    # æ¯200ä¸ªepochï¼Œæ‰“å°lossä¿¡æ¯
    if epoch % 20 == 0:
        print('epoch:', epoch, 'loss:', float(loss))

# é¢„æµ‹éƒ¨åˆ†
print("*******predict*******")
# xxåœ¨-3åˆ°3ä¹‹é—´ä»¥æ­¥é•¿ä¸º0.01ï¼Œyyåœ¨-3åˆ°3ä¹‹é—´ä»¥æ­¥é•¿0.01,ç”Ÿæˆé—´éš”æ•°å€¼ç‚¹
xx, yy = np.mgrid[-3:3:.1, -3:3:.1]
# å°†xx, yyæ‹‰ç›´ï¼Œå¹¶åˆå¹¶é…å¯¹ä¸ºäºŒç»´å¼ é‡ï¼Œç”ŸæˆäºŒç»´åæ ‡ç‚¹
grid = np.c_[xx.ravel(), yy.ravel()]
grid = tf.cast(grid, tf.float32)
# å°†ç½‘æ ¼åæ ‡ç‚¹å–‚å…¥ç¥ç»ç½‘ç»œï¼Œè¿›è¡Œé¢„æµ‹ï¼Œprobsä¸ºè¾“å‡º
probs = []
for x_predict in grid:
    # ä½¿ç”¨è®­ç»ƒå¥½çš„å‚æ•°è¿›è¡Œé¢„æµ‹
    h1 = tf.matmul([x_predict], w1) + b1
    h1 = tf.nn.relu(h1)
    y = tf.matmul(h1, w2) + b2  # yä¸ºé¢„æµ‹ç»“æœ
    probs.append(y)

# å–ç¬¬0åˆ—ç»™x1ï¼Œå–ç¬¬1åˆ—ç»™x2
x1 = x_data[:, 0]
x2 = x_data[:, 1]
# probsçš„shapeè°ƒæ•´æˆxxçš„æ ·å­
probs = np.array(probs).reshape(xx.shape)
plt.scatter(x1, x2, color=np.squeeze(Y_c))
# æŠŠåæ ‡xx yyå’Œå¯¹åº”çš„å€¼probsæ”¾å…¥contour<[â€˜kÉ‘ntÊŠr]>å‡½æ•°ï¼Œç»™probså€¼ä¸º0.5çš„æ‰€æœ‰ç‚¹ä¸Šè‰²  pltç‚¹showå æ˜¾ç¤ºçš„æ˜¯çº¢è“ç‚¹çš„åˆ†ç•Œçº¿
plt.contour(xx, yy, probs, levels=[.5])
plt.show()

# è¯»å…¥çº¢è“ç‚¹ï¼Œç”»å‡ºåˆ†å‰²çº¿ï¼ŒåŒ…å«æ­£åˆ™åŒ–
# ä¸æ¸…æ¥šçš„æ•°æ®ï¼Œå»ºè®®printå‡ºæ¥æŸ¥çœ‹ 
```

ä¸‹å›¾åˆ†åˆ«æ˜¯æœªæ­£åˆ™åŒ–çš„ç»“æœå’Œæ­£åˆ™åŒ–åçš„ç»“æœ

![image-20210915202811114](img/image-20210915202811114.png)

![image-20210915203110534](img/image-20210915203110534.png)

## ä¼˜åŒ–å™¨





# ç¬¬3è®² ç¥ç»ç½‘ç»œæ­å»ºå…«è‚¡

## tf.keras æ­å»ºç¥ç»ç½‘ç»œå…«è‚¡

https://tensorflow.google.cn/api_docs/python/tf



1.import

å¯¼å…¥ç›¸å…³åº“

2.train,test

æŒ‡å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†

3.model=tf.keras.models.Sequential

åœ¨Sequential()ä¸­æ­å»ºç½‘ç»œç»“æ„,é€å±‚æè¿°æ¯å±‚ç½‘ç»œ

4.model.compile

åœ¨compile()ä¸­é…ç½®è®­ç»ƒæ–¹æ³•,å‘ŠçŸ¥è®­ç»ƒæ—¶é€‰æ‹©çº³å®—ä¼˜åŒ–å™¨,é€‰æ‹©å“ªä¸ªæŸå¤±å‡½æ•°,é€‰æ‹©å“ªç§è¯„æµ‹æŒ‡æ ‡

5.model.fit

åœ¨fit()ä¸­å‘ŠçŸ¥è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„è¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾,å‘ŠçŸ¥æ¯ä¸ªbatchæ˜¯å¤šå°‘,å‘ŠçŸ¥è¦è¿­ä»£å¤šå°‘æ¬¡æ•°æ®é›†

6.model.summary

ç”¨summary()æ‰“å°å‡ºç½‘ç»œçš„ç»“æ„å’Œå‚æ•°ç»Ÿè®¡

### 1. model=tf.keras.models.Sequential

model=tf.keras.models.Sequential([ç½‘ç»œç»“æ„]),æè¿°å„å±‚ç½‘ç»œç»“æ„

ç½‘ç»œç»“æ„ä¸¾ä¾‹:

- æ‹‰ç›´å±‚:

  ```python
  tf.keras.layers.Flatten()# æŠŠè¾“å…¥ç‰¹å¾æ‹‰ç›´å˜æˆä¸€ç»´æ•°ç»„
  ```

  

- å…¨è¿æ¥å±‚:

  ```python
  tf.keras.layers.Dense(ç¥ç»å…ƒä¸ªæ•°,activation="æ¿€æ´»å‡½æ•°",
  						kernel_regularizer=å“ªç§æ­£åˆ™åŒ–)
  # activation(å­—ç¬¦ä¸²ç»™å‡º)å¯é€‰:relu/softmax/sigmoid/tanh
  # kernel_regularizerå¯é€‰:tf.keras.regularizer.l1()/tf.keras.regularizer.l1()
  ```

- å·ç§¯å±‚:

- LATMå±‚:

### 2. model.compile

```python
model.compile(optimizer=ä¼˜åŒ–å™¨,loss=æŸå¤±å‡½æ•°,metrics=["å‡†ç¡®ç‡"])
```

**Compile ç”¨äºé…ç½®ç¥ç»ç½‘ç»œçš„è®­ç»ƒæ–¹æ³•ï¼Œå‘ŠçŸ¥è®­ç»ƒæ—¶ä½¿ç”¨çš„ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°å’Œå‡†ç¡®ç‡è¯„æµ‹æ ‡å‡†ã€‚**

**optimizer å¯ä»¥æ˜¯å­—ç¬¦ä¸²å½¢å¼ç»™å‡ºçš„ä¼˜åŒ–å™¨åå­—ï¼Œä¹Ÿå¯ä»¥æ˜¯å‡½æ•°å½¢å¼ï¼Œä½¿ç”¨å‡½æ•° å½¢å¼å¯ä»¥è®¾ç½®å­¦ä¹ ç‡ã€åŠ¨é‡å’Œè¶…å‚æ•°ã€‚** 

å¯é€‰é¡¹åŒ…æ‹¬ï¼š

â€˜sgdâ€™or tf.optimizers.SGD( lr=å­¦ä¹ ç‡, decay=å­¦ä¹ ç‡è¡°å‡ç‡, momentum=åŠ¨é‡å‚æ•°)

â€˜adagradâ€™or tf.keras.optimizers.Adagrad(lr=å­¦ä¹ ç‡, decay=å­¦ä¹ ç‡è¡°å‡ç‡)

â€˜adadeltaâ€™or tf.keras.optimizers.Adadelta(lr=å­¦ä¹ ç‡, decay=å­¦ä¹ ç‡è¡°å‡ç‡)

â€˜adamâ€™or tf.keras.optimizers.Adam (lr=å­¦ä¹ ç‡, decay=å­¦ä¹ ç‡è¡°å‡ç‡)

**Loss å¯ä»¥æ˜¯å­—ç¬¦ä¸²å½¢å¼ç»™å‡ºçš„æŸå¤±å‡½æ•°çš„åå­—ï¼Œä¹Ÿå¯ä»¥æ˜¯å‡½æ•°å½¢å¼ã€‚** 

å¯é€‰é¡¹åŒ…æ‹¬ï¼š 

â€˜mseâ€™or tf.keras.losses.MeanSquaredError()

â€˜sparse_categorical_crossentropy or tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)

**from_logitså‚æ•°è¡¨ç¤ºæ˜¯å¦æ˜¯åŸå§‹è¾“å‡º**,æŸå¤±å‡½æ•°å¸¸éœ€è¦ç»è¿‡ softmax ç­‰å‡½æ•°å°†è¾“å‡ºè½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ã€‚from_logits åˆ™ç”¨æ¥æ ‡æ³¨è¯¥æŸå¤±å‡½æ•°æ˜¯å¦éœ€è¦è½¬æ¢ä¸ºæ¦‚ç‡çš„å½¢å¼ï¼Œå– False æ—¶è¡¨ ç¤ºè½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œå– True æ—¶è¡¨ç¤ºæ²¡æœ‰è½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œç›´æ¥è¾“å‡ºã€‚

**Metrics æ ‡æ³¨ç½‘ç»œè¯„æµ‹æŒ‡æ ‡ã€‚**

å¯é€‰é¡¹åŒ…æ‹¬ï¼š â€˜accuracyâ€™ï¼šy~_~å’Œ y éƒ½æ˜¯æ•°å€¼ï¼Œå¦‚ y_=[1] y=[1]ã€‚

â€˜categorical_accuracyâ€™ï¼šy~_~å’Œ y éƒ½æ˜¯ä»¥ç‹¬çƒ­ç å’Œæ¦‚ç‡åˆ†å¸ƒè¡¨ç¤ºã€‚ å¦‚ y~_~=[0, 1, 0], y=[0.256, 0.695, 0.048]ã€‚
â€˜sparse_ categorical_accuracyâ€™ï¼šy~_~æ˜¯ä»¥æ•°å€¼å½¢å¼ç»™å‡ºï¼Œy æ˜¯ä»¥æ¦‚ç‡åˆ†å¸ƒå½¢å¼ç»™å‡ºã€‚
å¦‚ y_=[1],y=[0.256, 0.695, 0.048]ã€‚

### 3. model.fit

fitå‡½æ•°ç”¨äºæ‰§è¡Œè®­ç»ƒè¿‡ç¨‹

```python
model.fit(è®­ç»ƒé›†çš„è¾“å…¥ç‰¹å¾ï¼Œ è®­ç»ƒé›†çš„æ ‡ç­¾ï¼Œ batch_size, epochs, 
	validation_data = (æµ‹è¯•é›†çš„è¾“å…¥ç‰¹å¾ï¼Œæµ‹è¯•é›†çš„æ ‡ç­¾)ï¼Œ 
	validataion_split = ä»æµ‹è¯•é›†åˆ’åˆ†å¤šå°‘æ¯”ä¾‹ç»™è®­ç»ƒé›†ï¼Œ # è¿™ä¸€ä¸ªå‚æ•°ä¸ä¸Šä¸€ä¸ªäºŒé€‰ä¸€
	validation_freq = æµ‹è¯•çš„ epoch é—´éš”æ¬¡æ•°)
```

### 4. model.summary

summaryç”¨äºæ‰“å°ç½‘ç»œç»“æ„å’Œå‚æ•°ç»Ÿè®¡

![image-20210917102015818](img/image-20210917102015818.png)

## é¸¢å°¾èŠ±åˆ†ç±»å…­æ­¥æ³•å®ç°

```python
# S1.å¯¼åº“
import tensorflow as tf
from sklearn import datasets
import numpy as np

# S2.å¯¼æ•°æ®
x_train = datasets.load_iris().data
y_train = datasets.load_iris().target

# æ•°æ®é›†ä¹±åº
np.random.seed(116)
np.random.shuffle(x_train)
np.random.seed(116)
np.random.shuffle(y_train)
tf.random.set_seed(116)

# S3.ç¥ç»ç½‘ç»œç»“æ„
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(3, activation='softmax',
	kernel_regularizer=tf.keras.regularizers.l2())
])

# S4.é…ç½®ä¼˜åŒ–å™¨/æŸå¤±å‡½æ•°/å‡†ç¡®ç‡è¯„æµ‹æŒ‡æ ‡
model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1), 
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['sparse_categorical_accuracy'])
# S5.æ‰§è¡Œè®­ç»ƒ
model.fit(x_train, y_train, batch_size=32, epochs=500,
          validation_split=0.2, validation_freq=20)

# S6.æ‰“å°ç½‘ç»œç»“æ„å’Œå‚æ•°ç»Ÿè®¡
model.summary()
```

## éé¡ºåºç¥ç»ç½‘ç»œç»“æ„æ­å»º

ä½¿ç”¨ Sequential å¯ä»¥å¿«é€Ÿæ­å»ºç½‘ç»œç»“æ„ï¼Œä½†æ˜¯å¦‚æœç½‘ç»œåŒ…å«è·³è¿ç­‰å…¶ä»–å¤æ‚ç½‘ç»œç»“æ„ï¼ŒSequential å°±æ— æ³•è¡¨ç¤ºäº†ã€‚è¿™å°±éœ€è¦ä½¿ç”¨ class æ¥å£°æ˜ç½‘ç»œç»“æ„ã€‚

\__init__( ) å®šä¹‰æ‰€éœ€ç½‘ç»œç»“æ„å—

call( ) å†™å‡ºå‰å‘ä¼ æ’­

```python
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras import Model
from sklearn import datasets
import numpy as np

x_train = datasets.load_iris().data
y_train = datasets.load_iris().target

np.random.seed(116)
np.random.shuffle(x_train)
np.random.seed(116)
np.random.shuffle(y_train)
tf.random.set_seed(116)

class IrisModel(Model):
    def __init__(self): # å®ä¾‹åŒ–ä¸€ä¸ªç±»æ—¶ä¼šè‡ªåŠ¨è°ƒç”¨__init__æ–¹æ³•
        super(IrisModel, self).__init__()   # superå‡½æ•°è°ƒç”¨çˆ¶ç±»çš„æ–¹æ³•
        self.d1 = Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2())

    def call(self, x):  # callå‡½æ•°ä¸­å®ç°å‰å‘ä¼ æ’­,override
        y = self.d1(x)  # ?self.d1(x)ä¸ºä»€ä¹ˆæ˜¯å‡½æ•°
        return y

model = IrisModel() # ?å®ä¾‹åŒ–åcallå‡½æ•°è‡ªåŠ¨è°ƒç”¨äº†å—?

model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['sparse_categorical_accuracy'])

model.fit(x_train, y_train, batch_size=32, epochs=500, validation_split=0.2, validation_freq=20)
model.summary()
```

### æ‰‹å†™æ•°å­—è¯†åˆ«

```python
import tensorflow as tf

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0	# x_train.shapeä¸º(60000,28,28)

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),	# å°†è¾“å…¥å±‚æ‹‰ç›´
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['sparse_categorical_accuracy'])

model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)
model.summary()
```

classæ–¹å¼å®ç°

```python
import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras import Model

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0


class MnistModel(Model):
    def __init__(self):
        super(MnistModel, self).__init__()
        self.flatten = Flatten()
        self.d1 = Dense(128, activation='relu')
        self.d2 = Dense(10, activation='softmax')

    def call(self, x):
        x = self.flatten(x)
        x = self.d1(x)
        y = self.d2(x)
        return y


model = MnistModel()

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['sparse_categorical_accuracy'])

model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)
model.summary()
```

# ç¬¬4è®² ç¥ç»ç½‘ç»œå…«è‚¡åŠŸèƒ½æ‰©å±•

## è‡ªåˆ¶æ•°æ®é›†

è¯»å–åŸæ–‡ä»¶å¹¶ä¿å­˜å±‚å¯ä»¥ç›´æ¥è¯»å–çš„å¾…è®­ç»ƒæ•°æ®

```python
import tensorflow as tf
from PIL import Image
import numpy as np
import os

train_path = './mnist_image_label/mnist_train_jpg_60000/'
train_txt = './mnist_image_label/mnist_train_jpg_60000.txt'
x_train_savepath = './mnist_image_label/mnist_x_train.npy'
y_train_savepath = './mnist_image_label/mnist_y_train.npy'

test_path = './mnist_image_label/mnist_test_jpg_10000/'
test_txt = './mnist_image_label/mnist_test_jpg_10000.txt'
x_test_savepath = './mnist_image_label/mnist_x_test.npy'
y_test_savepath = './mnist_image_label/mnist_y_test.npy'


def generateds(path, txt):
    f = open(txt, 'r')  # ä»¥åªè¯»å½¢å¼æ‰“å¼€txtæ–‡ä»¶
    contents = f.readlines()  # è¯»å–æ–‡ä»¶ä¸­æ‰€æœ‰è¡Œ
    f.close()  # å…³é—­txtæ–‡ä»¶
    x, y_ = [], []  # å»ºç«‹ç©ºåˆ—è¡¨
    for content in contents:  # é€è¡Œå–å‡º
        value = content.split()  # ä»¥ç©ºæ ¼åˆ†å¼€ï¼Œå›¾ç‰‡è·¯å¾„ä¸ºvalue[0] , æ ‡ç­¾ä¸ºvalue[1] , å­˜å…¥åˆ—è¡¨
        img_path = path + value[0]  # æ‹¼å‡ºå›¾ç‰‡è·¯å¾„å’Œæ–‡ä»¶å
        img = Image.open(img_path)  # è¯»å…¥å›¾ç‰‡
        img = np.array(img.convert('L'))  # å›¾ç‰‡å˜ä¸º8ä½å®½ç°åº¦å€¼çš„np.arrayæ ¼å¼
        img = img / 255.  # æ•°æ®å½’ä¸€åŒ– ï¼ˆå®ç°é¢„å¤„ç†ï¼‰
        x.append(img)  # å½’ä¸€åŒ–åçš„æ•°æ®ï¼Œè´´åˆ°åˆ—è¡¨x
        y_.append(value[1])  # æ ‡ç­¾è´´åˆ°åˆ—è¡¨y_
        print('loading : ' + content)  # æ‰“å°çŠ¶æ€æç¤º

    x = np.array(x)  # å˜ä¸ºnp.arrayæ ¼å¼
    y_ = np.array(y_)  # å˜ä¸ºnp.arrayæ ¼å¼
    y_ = y_.astype(np.int64)  # å˜ä¸º64ä½æ•´å‹
    return x, y_  # è¿”å›è¾“å…¥ç‰¹å¾xï¼Œè¿”å›æ ‡ç­¾y_


if os.path.exists(x_train_savepath) and os.path.exists(y_train_savepath) and os.path.exists(
        x_test_savepath) and os.path.exists(y_test_savepath):
    print('-------------Load Datasets-----------------')
    x_train_save = np.load(x_train_savepath)
    y_train = np.load(y_train_savepath)
    x_test_save = np.load(x_test_savepath)
    y_test = np.load(y_test_savepath)
    x_train = np.reshape(x_train_save, (len(x_train_save), 28, 28))
    x_test = np.reshape(x_test_save, (len(x_test_save), 28, 28))
else:
    print('-------------Generate Datasets-----------------')
    x_train, y_train = generateds(train_path, train_txt)
    x_test, y_test = generateds(test_path, test_txt)

    print('-------------Save Datasets-----------------')
    x_train_save = np.reshape(x_train, (len(x_train), -1))
    x_test_save = np.reshape(x_test, (len(x_test), -1))
    np.save(x_train_savepath, x_train_save)
    np.save(y_train_savepath, y_train)
    np.save(x_test_savepath, x_test_save)
    np.save(y_test_savepath, y_test)

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['sparse_categorical_accuracy'])

model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)
model.summary()
```

## æ•°æ®å¢å¼º(å¢å¤§æ•°æ®é‡)

```
image_gen_train = tf.keras.preprocessing.image.ImageDataGenerator(
		rescale = æ‰€æœ‰æ•°æ®å°†ä¹˜ä»¥è¯¥æ•°å€¼
		rotation_range = éšæœºæ—‹è½¬è§’åº¦æ•°èŒƒå›´ 
		width_shift_range = éšæœºå®½åº¦åç§»é‡ 
		height_shift_range = éšæœºé«˜åº¦åç§»é‡ 
		æ°´å¹³ç¿»è½¬ï¼šhorizontal_flip = æ˜¯å¦éšæœºæ°´å¹³ç¿»è½¬ 
		éšæœºç¼©æ”¾ï¼šzoom_range = éšæœºç¼©æ”¾çš„èŒƒå›´ [1-nï¼Œ1+n] )
image_gen_train.fit(x_train)
```



```python
# æ˜¾ç¤ºåŸå§‹å›¾åƒå’Œå¢å¼ºåçš„å›¾åƒ
import tensorflow as tf
from matplotlib import pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)

image_gen_train = ImageDataGenerator(
    rescale=1. / 255,
    rotation_range=45,
    width_shift_range=.15,
    height_shift_range=.15,
    horizontal_flip=False,
    zoom_range=0.5
)
image_gen_train.fit(x_train)
print("xtrain",x_train.shape)
x_train_subset1 = np.squeeze(x_train[:12])
print("xtrain_subset1",x_train_subset1.shape)
print("xtrain",x_train.shape)
x_train_subset2 = x_train[:12]  # ä¸€æ¬¡æ˜¾ç¤º12å¼ å›¾ç‰‡
print("xtrain_subset2",x_train_subset2.shape)

fig = plt.figure(figsize=(20, 2))
plt.set_cmap('gray')
# æ˜¾ç¤ºåŸå§‹å›¾ç‰‡
for i in range(0, len(x_train_subset1)):
    ax = fig.add_subplot(1, 12, i + 1)
    ax.imshow(x_train_subset1[i])
fig.suptitle('Subset of Original Training Images', fontsize=20)
plt.show()

# æ˜¾ç¤ºå¢å¼ºåçš„å›¾ç‰‡
fig = plt.figure(figsize=(20, 2))
for x_batch in image_gen_train.flow(x_train_subset2, batch_size=12, shuffle=False):
    for i in range(0, 12):
        ax = fig.add_subplot(1, 12, i + 1)
        ax.imshow(np.squeeze(x_batch[i]))
    fig.suptitle('Augmented Images', fontsize=20)
    plt.show()
    break;
```

![image-20210918142428606](img/image-20210918142428606-16319462712491.png)

![image-20210918142454457](img/image-20210918142454457.png)

![image-20210918104632063](img/image-20210918104632063.png)

```python
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)  # ç»™æ•°æ®å¢åŠ ä¸€ä¸ªç»´åº¦,ä»(60000, 28, 28)reshapeä¸º(60000, 28, 28, 1)

image_gen_train = ImageDataGenerator(
    rescale=1. / 1.,  # å¦‚ä¸ºå›¾åƒï¼Œåˆ†æ¯ä¸º255æ—¶ï¼Œå¯å½’è‡³0ï½1
    rotation_range=45,  # éšæœº45åº¦æ—‹è½¬
    width_shift_range=.15,  # å®½åº¦åç§»
    height_shift_range=.15,  # é«˜åº¦åç§»
    horizontal_flip=False,  # æ°´å¹³ç¿»è½¬
    zoom_range=0.5  # å°†å›¾åƒéšæœºç¼©æ”¾é˜ˆé‡50ï¼…
)
image_gen_train.fit(x_train)

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['sparse_categorical_accuracy'])

model.fit(image_gen_train.flow(x_train, y_train, batch_size=32), epochs=5, validation_data=(x_test, y_test),
          validation_freq=1)
model.summary()
```

## æ–­ç‚¹ç»­è®­,å­˜å–æ¨¡å‹

![image-20210918110159689](img/image-20210918110159689.png)

```python
import tensorflow as tf
import os

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['sparse_categorical_accuracy'])

checkpoint_save_path = "./checkpoint/mnist.ckpt"
if os.path.exists(checkpoint_save_path + '.index'):
    print('-------------load the model-----------------')
    model.load_weights(checkpoint_save_path)

cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,
                                                 save_weights_only=True,
                                                 save_best_only=True)

history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1,
                    callbacks=[cp_callback])
model.summary()
```

## æå–å¯è®­ç»ƒå‚æ•°,å†™å…¥æ–‡æœ¬

![image-20210918111736634](img/image-20210918111736634.png)

```python
import tensorflow as tf
import os
import numpy as np
np.set_printoptions(threshold=np.inf)
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['sparse_categorical_accuracy'])
checkpoint_save_path = "./checkpoint/mnist.ckpt"
if os.path.exists(checkpoint_save_path + '.index'):
    print('-------------load the model-----------------')
    model.load_weights(checkpoint_save_path)
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,
                                                 save_weights_only=True,
                                                 save_best_only=True)
history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1,
                    callbacks=[cp_callback])
model.summary()
print(model.trainable_variables)
file = open('./weights.txt', 'w')
for v in model.trainable_variables:
    file.write(str(v.name) + '\n')
    file.write(str(v.shape) + '\n')
    file.write(str(v.numpy()) + '\n')
file.close()
```

## acc/losså¯è§†åŒ–,æŸ¥çœ‹è®­ç»ƒæ•ˆæœ

![image-20210918112229065](img/image-20210918112229065.png)

```python
import tensorflow as tf
import os
import numpy as np
from matplotlib import pyplot as plt

np.set_printoptions(threshold=np.inf)

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['sparse_categorical_accuracy'])

checkpoint_save_path = "./checkpoint/mnist.ckpt"
if os.path.exists(checkpoint_save_path + '.index'):
    print('-------------load the model-----------------')
    model.load_weights(checkpoint_save_path)

cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,
                                                 save_weights_only=True,
                                                 save_best_only=True)

history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1,
                    callbacks=[cp_callback])
model.summary()

print(model.trainable_variables)
file = open('./weights.txt', 'w')
for v in model.trainable_variables:
    file.write(str(v.name) + '\n')
    file.write(str(v.shape) + '\n')
    file.write(str(v.numpy()) + '\n')
file.close()

###############################################    show   ###############################################

# æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„accå’Œlossæ›²çº¿
acc = history.history['sparse_categorical_accuracy']
val_acc = history.history['val_sparse_categorical_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

plt.subplot(1, 2, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()
```

## è¾“å…¥å›¾ç‰‡,è¯†åˆ«æ•°å­—åº”ç”¨

![image-20210918112729488](img/image-20210918112729488.png)

```python
from PIL import Image
import numpy as np
import tensorflow as tf

model_save_path = './checkpoint/mnist.ckpt'

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')])
    
model.load_weights(model_save_path)

preNum = int(input("input the number of test pictures:"))

for i in range(preNum):
    image_path = input("the path of test picture:")
    img = Image.open(image_path)
    img = img.resize((28, 28), Image.ANTIALIAS)
    img_arr = np.array(img.convert('L'))

    img_arr = 255 - img_arr
                
    img_arr = img_arr / 255.0
    print("img_arr:",img_arr.shape)
    x_predict = img_arr[tf.newaxis, ...]
    print("x_predict:",x_predict.shape)
    result = model.predict(x_predict)
    
    pred = tf.argmax(result, axis=1)
    
    print('\n')
    tf.print(pred)
```

# ç¬¬5è®² CNN

